{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPNw67eJidgr"
      },
      "source": [
        "# **Transformers learn in-context by gradient descent**\n",
        "This specific notebook can be used to reproduce the non-linear regression task results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "FR8YNR-g9JXA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\Lib\\site-packages\\~umpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\Lib\\site-packages\\~l_dtypes'.\n",
            "  You can safely remove it manually.\n"
          ]
        }
      ],
      "source": [
        "#@title Imports external sources\n",
        "import os\n",
        "import io\n",
        "import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n",
        "import base64\n",
        "import zipfile\n",
        "import json\n",
        "import requests\n",
        "import matplotlib.pylab as pl\n",
        "import numpy as np\n",
        "import glob\n",
        "import requests\n",
        "import random as pyrandom\n",
        "from concurrent import futures\n",
        "from functools import partial\n",
        "from scipy.ndimage import rotate\n",
        "from IPython.display import Image, HTML, clear_output\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "import time\n",
        "from typing import Any, MutableMapping, NamedTuple, Tuple\n",
        "!pip install --quiet --upgrade tensorflow \n",
        "!pip install --quiet --upgrade jax\n",
        "!pip install --quiet --upgrade jaxlib \n",
        "import jax\n",
        "from jax import grad, jit, vmap\n",
        "import jax.numpy as jnp\n",
        "\n",
        "!pip install --quiet -U dm-haiku\n",
        "!pip install --quiet -U optax\n",
        "import haiku as hk\n",
        "import math\n",
        "!pip install --quiet -U ml_collections\n",
        "from ml_collections import config_dict\n",
        "import matplotlib.pylab as pl\n",
        "import matplotlib.colors as mcolors\n",
        "colors = pl.colormaps['Dark2'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "933ztM3DSREA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/self-organising-systems/transformers_learn_icl_by_gd'\n",
            "c:\\Users\\georg\\Desktop\\test_communication\\selforg_project\\self-organising-systems\\transformers_learn_icl_by_gd\n"
          ]
        }
      ],
      "source": [
        "#@title Import internal sources (from github)\n",
        "!git clone --quiet https://github.com/google-research/self-organising-systems.git /content/self-organising-systems > /dev/null 2>&1\n",
        "%cd /content/self-organising-systems/transformers_learn_icl_by_gd\n",
        "from src.transformer import Transformer\n",
        "from src.data import create_reg_data, create_weights\n",
        "from src.config import config\n",
        "from src.train import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "4KwAI4LZFfcF"
      },
      "outputs": [],
      "source": [
        "#@title Config\n",
        "num_seeds = 1 #@param {type:\"integer\"}\n",
        "\n",
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "config.local_usage = True\n",
        "config.size_distract = 0\n",
        "config.training_steps = 20000\n",
        "config.training_steps_gd = 20000\n",
        "config.use_softmax = False\n",
        "config.non_linear_reg_task = True\n",
        "\n",
        "####\n",
        "config.deq = True\n",
        "config.gd_deq = True\n",
        "####\n",
        "config.pre_train_gd = True\n",
        "config.train_gd_whitening = False\n",
        "config.train_gd_lr = True\n",
        "####\n",
        "\n",
        "config.layer_norm = False\n",
        "config.out_proj = False\n",
        "config.in_proj = False\n",
        "config.adam = True\n",
        "\n",
        "config.dataset_size = 10\n",
        "config.input_size = 39\n",
        "config.key_size = 40 #config.input_size + 1\n",
        "config.num_layers = 1\n",
        "config.num_heads = 1\n",
        "config.grad_clip_value = 100\n",
        "config.grad_clip_value_gd = 100\n",
        "config.lr = 0.001\n",
        "config.wd = 0.0\n",
        "config.init_scale = 0.002 / config.num_layers\n",
        "config.bs = 2048\n",
        "config.bs_gd_train = 2048\n",
        "config.gd_lr = 0.0003\n",
        "\n",
        "config.dropout_rate = 0.0\n",
        "data_creator = vmap(create_reg_data_sin,\n",
        "                    in_axes=(0, None, None, None, None, None),\n",
        "                    out_axes=0)\n",
        "data_creator_sin_test = vmap(create_reg_data_sin_test, in_axes=(0, None, None,\n",
        "                                                  None, None), out_axes=0)\n",
        "\n",
        "config.y_update = False\n",
        "config.input_range = 10\n",
        "config.seed = 0\n",
        "\n",
        "config.analyse = True\n",
        "config.input_mlp = True\n",
        "config.input_mlp_out_dim = 40\n",
        "config.widening_factor = 4\n",
        "config.sum_norm = False\n",
        "\n",
        "\n",
        "config.in_proj = True\n",
        "config.emb_size = 40\n",
        "config.num_seeds = num_seeds\n",
        "\n",
        "change_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "z9P7XZf7rh6l"
      },
      "outputs": [],
      "source": [
        "#@title Utils\n",
        "pl.rcParams.update({'font.size': 12})\n",
        "pl.rc('axes', labelsize=14)\n",
        "pl.rcParams.update({\n",
        "    \"text.usetex\": False,\n",
        "})\n",
        "\n",
        "import matplotlib.colors as mcolors\n",
        "colors = pl.colormaps['Dark2'] \n",
        "def np2pil(a):\n",
        "  if a.dtype in [np.float32, np.float64]:\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb') #GFile.open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=95)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "def grab_plot(close=True):\n",
        "  \"\"\"Return the current Matplotlib figure as an image.\"\"\"\n",
        "  fig = pl.gcf()\n",
        "  fig.canvas.draw()\n",
        "  img = np.array(fig.canvas.renderer._renderer)\n",
        "  a = np.float32(img[..., 3:]/255.0)\n",
        "  img = np.uint8(255*(1.0-a) + img[...,:3] * a)  # alpha\n",
        "  if close:\n",
        "    pl.close()\n",
        "  return img\n",
        "\n",
        "def display_learning(train, test=None, gt=None, inter=None, title=\"train\", \n",
        "                     title1=\"Trained TF\", title2=\"Test\", \n",
        "                     title3='Gradient descent', title4='Interpolated',\n",
        "                     y_label1 = 'L2 Norm', y_label2 = 'Cosine sim',\n",
        "                     y_lim_l=0,  y_lim_u=1, single_seeds= False,\n",
        "                     plot_title = None,\n",
        "                     y_lim_u2= 1., y_lim_l2=0.,  x_label = 'Training steps',   \n",
        "                     second_axis=False, color_add=0, rw=10, num_iter_os=None, \n",
        "                     allow_download=False, plot_num=1, two_plots=False, \n",
        "                     loc_first = 'upper left', label_title=\"Loss\",\n",
        "                     loc_sec='upper left', yscale_log=False, line=\"-\",\n",
        "                     color_axis=False, \n",
        "                     height=3.5, width = 4, ax1=None, ax2=None):\n",
        "  \n",
        "  \"\"\"Update learning curve image.\"\"\"\n",
        "\n",
        "  train_list = train\n",
        "  train = np.array(train)\n",
        "  num_seeds_train = train.shape[0]\n",
        "  train_std = np.std(train, axis=0)\n",
        "  train = np.mean(train, axis=0)\n",
        "  \n",
        "  if test is not None:\n",
        "    test_list = test\n",
        "    test_std = np.std(test, axis=0)\n",
        "    test = np.mean(test, axis=0)\n",
        "\n",
        "  if gt is not None:\n",
        "    gt_list = gt\n",
        "    gt_std = np.std(gt, axis=0)\n",
        "    gt = np.mean(gt, axis=0)\n",
        "\n",
        "  if inter is not None:\n",
        "    inter_list = inter\n",
        "    inter_std = np.std(inter, axis=0)\n",
        "    inter = np.mean(inter, axis=0)\n",
        "\n",
        "  if plot_num == 1:\n",
        "    fig, ax1 = pl.subplots()\n",
        "    ax1.set_xlabel(x_label)\n",
        "    fig.set_size_inches(width, height)\n",
        "\n",
        "  \n",
        "  if test is not None and not second_axis:\n",
        "    x_range = np.arange(0, num_iter_os, int(num_iter_os/len(test)))\n",
        "    if len(test_list) > 1:\n",
        "      if single_seeds:\n",
        "        for s in test_list:\n",
        "          ax1.plot(x_range, s, color=colors(0.1+color_add), alpha=0.2, linewidth='2')\n",
        "      else:\n",
        "        ax1.fill_between(x_range, test-test_std, test+test_std ,alpha=0.2, facecolor=colors(0.1+color_add))\n",
        "    ax1.plot(x_range[:len(test)], test, color=colors(0.1+color_add), label=title2,linewidth='3')\n",
        "    #test_avg = moving_average(test, rw)\n",
        "    #ax1.plot(x_range[:len(test_avg)], test_avg, color=colors(0.1+color_add), label=title2)\n",
        "      \n",
        "  if gt is not None:\n",
        "    if not second_axis:\n",
        "      x_range = np.arange(0, num_iter_os, int(num_iter_os/len(gt)))\n",
        "      #ax1.plot(x_range[:len(gt[:-rw])], gt[:-rw], color=colors(0.2+color_add), alpha=0.3)\n",
        "      #gt_avg = moving_average(gt, rw)\n",
        "      ax1.plot(x_range[:len(gt)],gt, color=colors(0.2+color_add), label=title3,linewidth='3')\n",
        "      if len(gt_list) > 1:\n",
        "        if single_seeds:\n",
        "          for s in gt_list:\n",
        "            ax1.plot(x_range, s, color=colors(0.2+color_add), alpha=0.2, linewidth='2', zorder=0)\n",
        "        else:\n",
        "          ax1.fill_between(x_range, gt-gt_std, gt+gt_std,alpha=0.2, facecolor=colors(0.2+color_add))\n",
        "    else:\n",
        "      x_range = np.arange(0, num_iter_os, int(num_iter_os/len(gt)))\n",
        "      ax1.plot(x_range, gt, color=colors(0.6+color_add), label=title3,linewidth='3')\n",
        "      if len(gt_list) > 1:\n",
        "        if single_seeds:\n",
        "          for s in gt_list:\n",
        "            ax1.plot(x_range, s, color=colors(0.6+color_add), alpha=0.3, linewidth='2', zorder=0)\n",
        "        else:\n",
        "          ax1.fill_between(x_range, gt-gt_std, gt+gt_std ,alpha=0.2, facecolor=colors(0.6+color_add))\n",
        "\n",
        "  if test is not None and second_axis:\n",
        "    x_range = np.arange(0, num_iter_os, int(num_iter_os/len(test)))\n",
        "    ax1.plot(x_range[:len(test[:-rw])], test[:-rw], color=colors(0.5+color_add), label=title2,linewidth='3')\n",
        "    #test_avg = moving_average(test, rw)\n",
        "    #ax1.plot(x_range[:len(test_avg)],test_avg, color=colors(0.5+color_add))\n",
        "    if len(test_list) > 1:\n",
        "      if single_seeds:\n",
        "        for s in test_list:\n",
        "          ax1.plot(x_range, s, color=colors(0.5+color_add), linewidth='2', alpha=0.3, zorder=0)\n",
        "      else:\n",
        "        ax1.fill_between(x_range, test-test_std, test+test_std ,alpha=0.2, facecolor=colors(0.5+color_add))\n",
        "\n",
        "  if inter is not None and not second_axis:\n",
        "    x_range = np.arange(0, num_iter_os, int(num_iter_os/len(inter)))\n",
        "    ax1.plot(x_range, inter, color=colors(0.4+color_add), label=title4, linewidth='3', zorder=10)\n",
        "    if len(inter_list) > 1:\n",
        "      if single_seeds:\n",
        "        for s in inter_list:\n",
        "          ax1.plot(x_range, s, color=colors(0.4+color_add), alpha=0.3, linewidth='2', zorder=0)\n",
        "      else:\n",
        "        ax1.fill_between(x_range, inter-inter_std, inter+inter_std ,alpha=0.2, facecolor=colors(0.4+color_add), zorder=1)\n",
        "    #inter_avg = moving_average(inter, rw)\n",
        "    #ax1.plot(x_range[:len(inter_avg)], inter_avg, color=colors(0.7+color_add), label=title4)\n",
        "\n",
        "\n",
        "  if second_axis:\n",
        "    if ax2 is None:\n",
        "      ax2 = ax1.twinx()\n",
        "    ax2.set_zorder(0)\n",
        "    ax1.set_zorder(1)\n",
        "    ax1.set_frame_on(False)\n",
        "    #train_avg = moving_average(train, rw)\n",
        "    #ax2.plot(train[:-rw], color=colors(0.1+color_add), alpha=0.3)\n",
        "    ax2.plot(x_range, train, color=colors(0.4+color_add), label=title1, linewidth='3')\n",
        "    ax2.plot(x_range, np.ones_like(train), \"--\", color=\"gray\", linewidth='0.7')\n",
        "    if len(train_list) > 1:\n",
        "      if single_seeds:\n",
        "        for s in train_list:\n",
        "          ax1.plot(x_range, s, line, color=colors(0.4+color_add), alpha=0.3, linewidth='2', zorder=0)\n",
        "      else:\n",
        "        ax2.fill_between(x_range, train-train_std, train+train_std ,alpha=0.2, facecolor=colors(0.4+color_add))\n",
        "\n",
        "    if color_axis:\n",
        "      ax2.yaxis.label.set_color(colors(0.4+color_add))\n",
        "    else:\n",
        "      legend2 = ax2.legend(loc='upper right', framealpha=0.99, facecolor='white')\n",
        "      legend2.set_zorder(100)\n",
        "    ax2.spines['top'].set_visible(False)\n",
        "  else:\n",
        "    #train_avg = moving_average(train, rw)\n",
        "    if line != \"-\":\n",
        "      ax1.scatter(x_range, train, s=[100 for _ in x_range], \n",
        "                  marker=\"+\", color=colors(0.3+color_add), alpha=1, label=title1, zorder=3, linewidths=3)\n",
        "    else:\n",
        "      ax1.plot(x_range, train, line, color=colors(0.3+color_add), label=title1, linewidth='3', zorder=11)\n",
        "    #ax1.plot(x_range[:len(train_avg)], train_avg, line, color=colors(0.3+color_add), label=title1)\n",
        "    if len(train_list) > 1:\n",
        "      if single_seeds:\n",
        "          for s in train_list:\n",
        "            ax1.plot(x_range, s, line, color=colors(0.3+color_add), alpha=0.3, linewidth='2', zorder=0)\n",
        "      else: \n",
        "        ax1.fill_between(x_range, train-train_std, train+train_std,\n",
        "                       alpha=0.5, facecolor=colors(0.3+color_add))\n",
        "\n",
        "    ax1.legend(loc='best', framealpha=1, facecolor='white')\n",
        "    ax1.spines['right'].set_visible(False)\n",
        "    legend = ax1.legend(loc='upper right', framealpha=0.99, facecolor='white')\n",
        "    legend.set_zorder(100)\n",
        "  \n",
        "  legend1 = ax1.legend(loc=loc_first, framealpha=0.99, facecolor='white')\n",
        "  legend1.set_zorder(100)\n",
        "  if second_axis:\n",
        "    ax2.set_ylabel(y_label2)\n",
        "    ax1.set_ylabel(y_label1)\n",
        "    ax1.set_ylim(y_lim_l, y_lim_u)\n",
        "    legend1 = ax1.legend(loc=loc_sec, framealpha=0.99, facecolor='white')\n",
        "    ax2.set_ylim(y_lim_l2, y_lim_u2)\n",
        "    ax1.set_ylim(bottom=0)\n",
        "  else:\n",
        "    pl.ylabel(label_title)\n",
        "    pl.ylim(y_lim_l, y_lim_u)\n",
        "  ax1.spines['top'].set_visible(False)\n",
        "  \n",
        "  if plot_title is not None:\n",
        "    pl.title(plot_title)\n",
        "    \n",
        "  if yscale_log:\n",
        "    ax1.set_yscale(\"log\")\n",
        "  #pl.title(title)\n",
        "  pl.tight_layout()\n",
        "\n",
        "  if allow_download:\n",
        "    if second_axis:\n",
        "      pl.savefig(\"sim.pdf\", format=\"pdf\")\n",
        "      %download_file sim.pdf\n",
        "    else:\n",
        "      pl.savefig(\"train.pdf\", format=\"pdf\")\n",
        "      %download_file train.pdf\n",
        "  else:\n",
        "    img = grab_plot()\n",
        "    display(Image(data=imencode(img, fmt='jpeg')), display_id=title)\n",
        "\n",
        "def sin_plot(preds, eval_data, other_preds = None, title=\"GD\", \n",
        "                y_lim_l=-0.25,  y_lim_u=0.4,\n",
        "                  title_other=\"Tr. TF\", allow_download=False):\n",
        "\n",
        "  pl.rcParams.update({'font.size': 12})\n",
        "  pl.rc('axes', labelsize=14)\n",
        "  pl.rcParams.update({\n",
        "      \"text.usetex\": False,\n",
        "  })\n",
        "\n",
        "  fig, ax1 = pl.subplots()\n",
        "  fig.set_size_inches(6, 3.5)\n",
        "  ax1.set_xlabel('x')\n",
        "  ax1.set_ylabel('y')\n",
        "  ax1.spines['right'].set_visible(False)\n",
        "  ax1.spines['top'].set_visible(False)\n",
        "\n",
        "  phase, amp = eval_data[2] \n",
        "  phase = phase[0][0]\n",
        "  amp = amp[0][0]\n",
        "\n",
        "  x = np.arange(-5, 5, 0.1)\n",
        "  y = np.sin(x + phase)*amp\n",
        "  ax1.plot(x, y, \"--\", color=\"black\", label=\"GT\")\n",
        "\n",
        "\n",
        "  x =  eval_data[0][0][:-1, 0]\n",
        "  y =  eval_data[0][0][:-1, 1]\n",
        "  ax1.plot(x, y, \"*\", color=\"red\", label=\"Data\")\n",
        "\n",
        "  x =  eval_data[1][:, 0]\n",
        "  for step in range(len(preds)):\n",
        "    prec = min([0.9, (step+1)/len(preds)])\n",
        "    sort_x = np.argsort(x)\n",
        "    if step >= 1:\n",
        "      ax1.plot(x[sort_x], preds[step][sort_x], \"-\", color=colors(0.1),\n",
        "             alpha = 0.1 + prec, label=title + \" step \" + str(step))\n",
        "    elif step == 0:\n",
        "      ax1.plot(x[sort_x], preds[step][sort_x], \"-\", color=colors(0.1),\n",
        "             alpha = 0.1 + prec, label=title + \" init\")\n",
        "  if other_preds is not None:\n",
        "    for step in range(len(other_preds)):\n",
        "      prec = min([0.9, (step+1)/len(preds)])\n",
        "      sort_x = np.argsort(x)\n",
        "      if step >= 1:\n",
        "        ax1.plot(x[sort_x], other_preds[step][sort_x], \"-\", color=colors(0.3),\n",
        "              alpha = 0.1 + prec, label=title_other + \" step \" + str(step))\n",
        "      elif step == 0:\n",
        "        ax1.plot(x[sort_x], other_preds[step][sort_x], \"-\", color=colors(0.3),\n",
        "              alpha = 0.1 + prec, label=title_other + \" init\")\n",
        "    legend1 = ax1.legend(ncol=3, loc='upper right', framealpha=0.99, facecolor='white')\n",
        "  else:\n",
        "    legend1 = ax1.legend(ncol=2, loc='upper right', framealpha=0.99, facecolor='white')\n",
        "\n",
        "  pl.ylim(y_lim_l, y_lim_u)\n",
        "  \n",
        "  pl.tight_layout()\n",
        "  if allow_download:\n",
        "    pl.savefig(\"sine_wave.pdf\", format=\"pdf\")\n",
        "    %download_file sine_wave.pdf\n",
        "  else:\n",
        "    img = grab_plot()\n",
        "    display(Image(data=imencode(img, fmt='jpeg')), display_id=title)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "Li0HaIsW5BFQ"
      },
      "outputs": [],
      "source": [
        "#@title Lists\n",
        "loss_trans_list =  [[]  for _ in range(config.num_seeds)]\n",
        "losses_gd_list =  [[]  for _ in range(config.num_seeds)]\n",
        "p_norm_list =  [[]  for _ in range(config.num_seeds)]\n",
        "grad_norm_list =  [[]  for _ in range(config.num_seeds)]\n",
        "cos_sim_list  =  [[]  for _ in range(config.num_seeds)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "7s4jr3OBpaq9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss of trained MLP + GD (on the ouput head):  0 0.03258689\n",
            "Loss of trained MLP + GD (on the ouput head):  100 0.02577083\n",
            "Loss of trained MLP + GD (on the ouput head):  200 0.025135387\n",
            "Loss of trained MLP + GD (on the ouput head):  300 0.0238984\n",
            "Loss of trained MLP + GD (on the ouput head):  400 0.023369558\n",
            "Loss of trained MLP + GD (on the ouput head):  500 0.02004299\n",
            "Loss of trained MLP + GD (on the ouput head):  600 0.016895562\n",
            "Loss of trained MLP + GD (on the ouput head):  700 0.0161344\n",
            "Loss of trained MLP + GD (on the ouput head):  800 0.015745377\n",
            "Loss of trained MLP + GD (on the ouput head):  900 0.015348631\n",
            "Loss of trained MLP + GD (on the ouput head):  1000 0.01468886\n",
            "Loss of trained MLP + GD (on the ouput head):  1100 0.013885361\n",
            "Loss of trained MLP + GD (on the ouput head):  1200 0.012229692\n",
            "Loss of trained MLP + GD (on the ouput head):  1300 0.009192853\n",
            "Loss of trained MLP + GD (on the ouput head):  1400 0.007371236\n",
            "Loss of trained MLP + GD (on the ouput head):  1500 0.0060764737\n",
            "Loss of trained MLP + GD (on the ouput head):  1600 0.0051560174\n",
            "Loss of trained MLP + GD (on the ouput head):  1700 0.0044361297\n",
            "Loss of trained MLP + GD (on the ouput head):  1800 0.0039611254\n",
            "Loss of trained MLP + GD (on the ouput head):  1900 0.0037715032\n",
            "Loss of trained MLP + GD (on the ouput head):  2000 0.0034807432\n",
            "Loss of trained MLP + GD (on the ouput head):  2100 0.003380523\n",
            "Loss of trained MLP + GD (on the ouput head):  2200 0.0033353597\n",
            "Loss of trained MLP + GD (on the ouput head):  2300 0.0032661366\n",
            "Loss of trained MLP + GD (on the ouput head):  2400 0.003213252\n",
            "Loss of trained MLP + GD (on the ouput head):  2500 0.003209218\n",
            "Loss of trained MLP + GD (on the ouput head):  2600 0.0032724207\n",
            "Loss of trained MLP + GD (on the ouput head):  2700 0.0031183173\n",
            "Loss of trained MLP + GD (on the ouput head):  2800 0.0030882228\n",
            "Loss of trained MLP + GD (on the ouput head):  2900 0.0030834721\n",
            "Loss of trained MLP + GD (on the ouput head):  3000 0.0030588962\n",
            "Loss of trained MLP + GD (on the ouput head):  3100 0.0030000077\n",
            "Loss of trained MLP + GD (on the ouput head):  3200 0.003059719\n",
            "Loss of trained MLP + GD (on the ouput head):  3300 0.0029563121\n",
            "Loss of trained MLP + GD (on the ouput head):  3400 0.0029327758\n",
            "Loss of trained MLP + GD (on the ouput head):  3500 0.0029360596\n",
            "Loss of trained MLP + GD (on the ouput head):  3600 0.0028824657\n",
            "Loss of trained MLP + GD (on the ouput head):  3700 0.0028896793\n",
            "Loss of trained MLP + GD (on the ouput head):  3800 0.0028385057\n",
            "Loss of trained MLP + GD (on the ouput head):  3900 0.0028865025\n",
            "Loss of trained MLP + GD (on the ouput head):  4000 0.00281333\n",
            "Loss of trained MLP + GD (on the ouput head):  4100 0.0028449704\n",
            "Loss of trained MLP + GD (on the ouput head):  4200 0.0027676441\n",
            "Loss of trained MLP + GD (on the ouput head):  4300 0.0027470575\n",
            "Loss of trained MLP + GD (on the ouput head):  4400 0.0027513904\n",
            "Loss of trained MLP + GD (on the ouput head):  4500 0.0027421045\n",
            "Loss of trained MLP + GD (on the ouput head):  4600 0.0026937863\n",
            "Loss of trained MLP + GD (on the ouput head):  4700 0.0026919905\n",
            "Loss of trained MLP + GD (on the ouput head):  4800 0.0026581017\n",
            "Loss of trained MLP + GD (on the ouput head):  4900 0.0026638254\n",
            "Loss of trained MLP + GD (on the ouput head):  5000 0.00264162\n",
            "Loss of trained MLP + GD (on the ouput head):  5100 0.0026215394\n",
            "Loss of trained MLP + GD (on the ouput head):  5200 0.0026313956\n",
            "Loss of trained MLP + GD (on the ouput head):  5300 0.002615382\n",
            "Loss of trained MLP + GD (on the ouput head):  5400 0.0025960056\n",
            "Loss of trained MLP + GD (on the ouput head):  5500 0.0026521413\n",
            "Loss of trained MLP + GD (on the ouput head):  5600 0.0025684163\n",
            "Loss of trained MLP + GD (on the ouput head):  5700 0.0026074885\n",
            "Loss of trained MLP + GD (on the ouput head):  5800 0.002531022\n",
            "Loss of trained MLP + GD (on the ouput head):  5900 0.0025372473\n",
            "Loss of trained MLP + GD (on the ouput head):  6000 0.002504067\n",
            "Loss of trained MLP + GD (on the ouput head):  6100 0.0026426339\n",
            "Loss of trained MLP + GD (on the ouput head):  6200 0.0025007068\n",
            "Loss of trained MLP + GD (on the ouput head):  6300 0.0024788922\n",
            "Loss of trained MLP + GD (on the ouput head):  6400 0.0024661315\n",
            "Loss of trained MLP + GD (on the ouput head):  6500 0.0025071334\n",
            "Loss of trained MLP + GD (on the ouput head):  6600 0.0024431807\n",
            "Loss of trained MLP + GD (on the ouput head):  6700 0.0024153735\n",
            "Loss of trained MLP + GD (on the ouput head):  6800 0.0024397376\n",
            "Loss of trained MLP + GD (on the ouput head):  6900 0.0024404463\n",
            "Loss of trained MLP + GD (on the ouput head):  7000 0.0023855679\n",
            "Loss of trained MLP + GD (on the ouput head):  7100 0.0023991296\n",
            "Loss of trained MLP + GD (on the ouput head):  7200 0.0023822314\n",
            "Loss of trained MLP + GD (on the ouput head):  7300 0.0023787308\n",
            "Loss of trained MLP + GD (on the ouput head):  7400 0.0023665135\n",
            "Loss of trained MLP + GD (on the ouput head):  7500 0.0023409901\n",
            "Loss of trained MLP + GD (on the ouput head):  7600 0.0023082083\n",
            "Loss of trained MLP + GD (on the ouput head):  7700 0.0022975295\n",
            "Loss of trained MLP + GD (on the ouput head):  7800 0.002294512\n",
            "Loss of trained MLP + GD (on the ouput head):  7900 0.0023001984\n",
            "Loss of trained MLP + GD (on the ouput head):  8000 0.0022623753\n",
            "Loss of trained MLP + GD (on the ouput head):  8100 0.0022525135\n",
            "Loss of trained MLP + GD (on the ouput head):  8200 0.002234443\n",
            "Loss of trained MLP + GD (on the ouput head):  8300 0.002220086\n",
            "Loss of trained MLP + GD (on the ouput head):  8400 0.0022133263\n",
            "Loss of trained MLP + GD (on the ouput head):  8500 0.0021904418\n",
            "Loss of trained MLP + GD (on the ouput head):  8600 0.0021745972\n",
            "Loss of trained MLP + GD (on the ouput head):  8700 0.0021927657\n",
            "Loss of trained MLP + GD (on the ouput head):  8800 0.0021431516\n",
            "Loss of trained MLP + GD (on the ouput head):  8900 0.002185617\n",
            "Loss of trained MLP + GD (on the ouput head):  9000 0.0021416913\n",
            "Loss of trained MLP + GD (on the ouput head):  9100 0.0021235715\n",
            "Loss of trained MLP + GD (on the ouput head):  9200 0.0020973913\n",
            "Loss of trained MLP + GD (on the ouput head):  9300 0.0020665547\n",
            "Loss of trained MLP + GD (on the ouput head):  9400 0.0020677627\n",
            "Loss of trained MLP + GD (on the ouput head):  9500 0.0020406605\n",
            "Loss of trained MLP + GD (on the ouput head):  9600 0.0020426132\n",
            "Loss of trained MLP + GD (on the ouput head):  9700 0.0020294574\n",
            "Loss of trained MLP + GD (on the ouput head):  9800 0.0019820747\n",
            "Loss of trained MLP + GD (on the ouput head):  9900 0.0019740048\n",
            "Loss of trained MLP + GD (on the ouput head):  10000 0.001955431\n",
            "Loss of trained MLP + GD (on the ouput head):  10100 0.0019287262\n",
            "Loss of trained MLP + GD (on the ouput head):  10200 0.0019403785\n",
            "Loss of trained MLP + GD (on the ouput head):  10300 0.0018919811\n",
            "Loss of trained MLP + GD (on the ouput head):  10400 0.0018762182\n",
            "Loss of trained MLP + GD (on the ouput head):  10500 0.0018546504\n",
            "Loss of trained MLP + GD (on the ouput head):  10600 0.0018390333\n",
            "Loss of trained MLP + GD (on the ouput head):  10700 0.0018570105\n",
            "Loss of trained MLP + GD (on the ouput head):  10800 0.0018243302\n",
            "Loss of trained MLP + GD (on the ouput head):  10900 0.0018032767\n",
            "Loss of trained MLP + GD (on the ouput head):  11000 0.0018033271\n",
            "Loss of trained MLP + GD (on the ouput head):  11100 0.0017684523\n",
            "Loss of trained MLP + GD (on the ouput head):  11200 0.0017660778\n",
            "Loss of trained MLP + GD (on the ouput head):  11300 0.0017599019\n",
            "Loss of trained MLP + GD (on the ouput head):  11400 0.0017429289\n",
            "Loss of trained MLP + GD (on the ouput head):  11500 0.0017257582\n",
            "Loss of trained MLP + GD (on the ouput head):  11600 0.0017160546\n",
            "Loss of trained MLP + GD (on the ouput head):  11700 0.0017043723\n",
            "Loss of trained MLP + GD (on the ouput head):  11800 0.001700267\n",
            "Loss of trained MLP + GD (on the ouput head):  11900 0.0016793023\n",
            "Loss of trained MLP + GD (on the ouput head):  12000 0.0016777171\n",
            "Loss of trained MLP + GD (on the ouput head):  12100 0.0016502386\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     13\u001b[0m     params_c \u001b[38;5;241m=\u001b[39m create_weights(config\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mdataset_size, \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     14\u001b[0m                               jnp\u001b[38;5;241m.\u001b[39mones([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39minput_size])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \n\u001b[0;32m     15\u001b[0m                               lin_diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, gd_deq\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgd_deq,\n\u001b[0;32m     16\u001b[0m                               num_layers\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_layers, \n\u001b[0;32m     17\u001b[0m                               input_mlp_rnd\u001b[38;5;241m=\u001b[39meval_rng \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39minput_mlp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpre_train_gd:\n\u001b[1;32m---> 19\u001b[0m       params_c, eval_rng \u001b[38;5;241m=\u001b[39m \u001b[43mpre_train_gd_hps\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m data_creator(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(eval_rng, num\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbs),\n\u001b[0;32m     21\u001b[0m                              config\u001b[38;5;241m.\u001b[39minput_size,\n\u001b[0;32m     22\u001b[0m                              config\u001b[38;5;241m.\u001b[39mdataset_size,\n\u001b[0;32m     23\u001b[0m                              config\u001b[38;5;241m.\u001b[39msize_distract,\n\u001b[0;32m     24\u001b[0m                              config\u001b[38;5;241m.\u001b[39minput_range,\n\u001b[0;32m     25\u001b[0m                              config\u001b[38;5;241m.\u001b[39mweight_scale)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mtraining_steps):\n",
            "File \u001b[1;32mc:\\Users\\georg\\Desktop\\test_communication\\selforg_project\\self-organising-systems\\transformers_learn_icl_by_gd\\src\\train.py:577\u001b[0m, in \u001b[0;36mpre_train_gd_hps\u001b[1;34m(eval_rng, params_gd)\u001b[0m\n\u001b[0;32m    575\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss of trained MLP + GD (on the ouput head): \u001b[39m\u001b[38;5;124m'\u001b[39m, step, losses_gd)\n\u001b[0;32m    576\u001b[0m   aug_gradients \u001b[38;5;241m=\u001b[39m gradient_manipulation(gradients, config\u001b[38;5;241m.\u001b[39mkey_size)\n\u001b[1;32m--> 577\u001b[0m   updates, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m   params_gd \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params_gd, updates)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m params_gd, data_rng\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\optax\\transforms\\_combining.py:75\u001b[0m, in \u001b[0;36mchain.<locals>.update_fn\u001b[1;34m(updates, state, params, **extra_args)\u001b[0m\n\u001b[0;32m     73\u001b[0m new_state \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(state, update_fns):\n\u001b[1;32m---> 75\u001b[0m   updates, new_s \u001b[38;5;241m=\u001b[39m fn(updates, s, params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args)\n\u001b[0;32m     76\u001b[0m   new_state\u001b[38;5;241m.\u001b[39mappend(new_s)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updates, \u001b[38;5;28mtuple\u001b[39m(new_state)\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\optax\\_src\\base.py:333\u001b[0m, in \u001b[0;36mwith_extra_args_support.<locals>.update\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(updates, state, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args):\n\u001b[0;32m    332\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m extra_args\n\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\optax\\transforms\\_clipping.py:91\u001b[0m, in \u001b[0;36mclip_by_global_norm.<locals>.update_fn\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fn\u001b[39m(updates, state, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m params\n\u001b[1;32m---> 91\u001b[0m   g_norm \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_algebra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m   \u001b[38;5;66;03m# TODO(b/163995078): revert back to the following (faster) implementation\u001b[39;00m\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# once analyzed how it affects backprop through update (e.g. meta-gradients)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# g_norm = jnp.maximum(max_norm, g_norm)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# updates = jax.tree.map(lambda t: (t / g_norm) * max_norm, updates)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m   trigger \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqueeze(g_norm \u001b[38;5;241m<\u001b[39m max_norm)\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\optax\\_src\\linear_algebra.py:38\u001b[0m, in \u001b[0;36mglobal_norm\u001b[1;34m(updates)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mglobal_norm\u001b[39m(updates: base\u001b[38;5;241m.\u001b[39mPyTree) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m chex\u001b[38;5;241m.\u001b[39mArray:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the global norm across a nested structure of tensors.\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msqrt(\n\u001b[1;32m---> 38\u001b[0m       \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs_sq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaves\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\optax\\_src\\linear_algebra.py:38\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mglobal_norm\u001b[39m(updates: base\u001b[38;5;241m.\u001b[39mPyTree) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m chex\u001b[38;5;241m.\u001b[39mArray:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the global norm across a nested structure of tensors.\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msqrt(\n\u001b[1;32m---> 38\u001b[0m       \u001b[38;5;28msum\u001b[39m(jnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mnumerics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs_sq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mleaves(updates))\n\u001b[0;32m     39\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\optax\\_src\\numerics.py:45\u001b[0m, in \u001b[0;36mabs_sq\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (np\u001b[38;5;241m.\u001b[39mndarray, jnp\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`abs_sq` accepts only NDarrays, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x)\u001b[38;5;241m.\u001b[39mreal\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:149\u001b[0m, in \u001b[0;36m_conj\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conj\u001b[39m(\u001b[38;5;28mself\u001b[39m: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Return the complex conjugate of the array.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m  Refer to :func:`jax.numpy.conj` for the full documentation.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufuncs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\georg\\miniconda3\\envs\\my_jax_env\\lib\\site-packages\\jax\\_src\\numpy\\ufuncs.py:3269\u001b[0m, in \u001b[0;36mconj\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   3266\u001b[0m \u001b[38;5;129m@export\u001b[39m\n\u001b[0;32m   3267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconj\u001b[39m(x: ArrayLike, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m   3268\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Alias of :func:`jax.numpy.conjugate`\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3269\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconjugate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Training\n",
        "eval_rng = jax.random.PRNGKey(10)\n",
        "for cur_seed in range(config.num_seeds):\n",
        "  if cur_seed == 1:\n",
        "    save_train_params = train_state.params\n",
        "  config.seed = cur_seed  \n",
        "  optimiser, train_state, _, rng = init()\n",
        "  rng, data_rng = jax.random.split(rng, 2)\n",
        "  if config.analyse:\n",
        "    if cur_seed == 0:\n",
        "      #lr_min, min_loss = scan_lrs(eval_rng, lin_diag=False)\n",
        "      #print('Best lr found for gradient descent: ', lr_min/config.dataset_size, min_loss)\n",
        "      params_c = create_weights(config.input_size, 1, config.dataset_size, 0.1,\n",
        "                                jnp.ones([1, 1, config.input_size])*0.0, \n",
        "                                lin_diag=False, gd_deq=config.gd_deq,\n",
        "                                num_layers=config.num_layers, \n",
        "                                input_mlp_rnd=eval_rng if config.input_mlp else None)\n",
        "      if config.pre_train_gd:\n",
        "        params_c, eval_rng = pre_train_gd_hps(eval_rng, params_c)\n",
        "  eval_data = data_creator(jax.random.split(eval_rng, num=config.bs),\n",
        "                               config.input_size,\n",
        "                               config.dataset_size,\n",
        "                               config.size_distract,\n",
        "                               config.input_range,\n",
        "                               config.weight_scale)\n",
        "  \n",
        "  for step in range(config.training_steps):\n",
        "    rng, data_rng = jax.random.split(data_rng, 2)\n",
        "    train_data = data_creator(jax.random.split(rng, num=config.bs), \n",
        "                              config.input_size,\n",
        "                              config.dataset_size,\n",
        "                              config.size_distract,\n",
        "                              config.input_range,\n",
        "                              config.weight_scale)\n",
        "    train_state, metrics = update(train_state, train_data, optimiser)\n",
        "    \n",
        "    #for params in train_state.params:\n",
        "    #  if \"mlp\" in params or 'emb' in params:\n",
        "    #    train_state.params[params] = params_c[params.replace(\"transformer\", \"Transformer_gd\")]\n",
        "    if step % 1000 == 0:\n",
        "      \n",
        "      loss_trans, _, _ = predict_test.apply(train_state.params, eval_rng,\n",
        "                                            eval_data, False)\n",
        "      \n",
        "      loss_trans_list[cur_seed].append(loss_trans)\n",
        "      if config.analyse:\n",
        "        losses_gd, _, _ = predict_test.apply(params_c, eval_rng, eval_data, True)\n",
        "        losses_gd_list[cur_seed].append(losses_gd)\n",
        "\n",
        "        #rng, data_rng, eval_rng = jax.random.split(data_rng, 3)\n",
        "        # Alignment Transformers and GD\n",
        "        cos_sim, w_norm, p_norm = analyse(eval_data, train_state, eval_rng, \n",
        "                                          params_c)\n",
        "        display((\"Current seed\", cur_seed, \"Training step\", step,\n",
        "                    \"Trained MLP with GD\", losses_gd.item(),\n",
        "                    \"Trained MLP with SA layer\", loss_trans.item(),\n",
        "                    \"Cosine sim TF vs GD\", cos_sim.item(), \n",
        "                    \"Diff predictions TF vs GD\", p_norm.item(), \n",
        "                    \"Diff gradients TF vs GD\", w_norm.item()),\n",
        "                    display_id=\"Cur met\")\n",
        "        \n",
        "        cos_sim_list[cur_seed].append(cos_sim)\n",
        "        p_norm_list[cur_seed].append(p_norm)\n",
        "        grad_norm_list[cur_seed].append(w_norm)\n",
        "\n",
        "if config.num_seeds == 1:\n",
        "  save_train_params = train_state.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tWjKF2SgrbTO"
      },
      "outputs": [],
      "source": [
        "#@title Visualize loss and alignment\n",
        "\n",
        "display_learning(loss_trans_list, test=losses_gd_list, y_lim_u=0.01, y_lim_l=0.0,\n",
        "                 rw=1, title=\"train.pdf\", allow_download=False, title3='GD', \n",
        "                 title1='Trained TF', title2='MLP + GD', single_seeds=True,\n",
        "                 num_iter_os=len(loss_trans_list[0])*1000)\n",
        "\n",
        "display_learning(cos_sim_list, grad_norm_list, p_norm_list, title1=\"Partial cosine\",\n",
        "                 title2=\"Partial diff\", y_lim_u=0.08, \n",
        "                 title3=\"Preds diff\", second_axis=True, \n",
        "                 y_lim_u2=1.09999,  color_add=0.2, loc_sec = 'lower left',\n",
        "                 y_lim_l2=0.5,\n",
        "                 rw=1, num_iter_os=len(cos_sim_list[0])*1000, title=\"sim.pdf\",\n",
        "                 allow_download=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MEvXnePuIiDr"
      },
      "outputs": [],
      "source": [
        "#@title Visualize functions before and after GD / SA layer\n",
        "\n",
        "rng, eval_rng = jax.random.split(eval_rng, 2)\n",
        "\n",
        "preds, eval_data = test_sin(params_c, rng, True)\n",
        "preds_tr, eval_data = test_sin(save_train_params, rng, False)\n",
        "sin_plot(preds, eval_data, other_preds=preds_tr, y_lim_l=-0.55, y_lim_u=0.75)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "name": "non_linear_regression",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my_jax_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
